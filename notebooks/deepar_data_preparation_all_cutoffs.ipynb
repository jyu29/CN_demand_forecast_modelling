{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import src.config as cf\n",
    "import src.utils as ut\n",
    "\n",
    "os.environ['HTTP_PROXY'] = \"http://proxy-internet-aws-eu.subsidia.org:3128\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://proxy-internet-aws-eu.subsidia.org:3128\"\n",
    "os.environ['no_proxy'] = \"169.254.169.254,127.0.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ENV = \"dev\"\n",
    "freq = 'W'\n",
    "prediction_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"../conf/prod.yml\" if RUN_ENV == \"prod\" else \"../conf/dev.yml\"\n",
    "config = cf.ProgramConfiguration(config_file, \"../conf/functional.yml\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = config.get_global_role_arn()\n",
    "\n",
    "bucket = config.get_train_bucket_input()\n",
    "prefix = \"test_deepAR\" # Your S3 test dir\n",
    "\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List cutoff week id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_files = sagemaker_session.list_s3_files(bucket, \"{}/full_scope/train_data_cutoff/\".format(prefix))\n",
    "l_cutoff_week_id = np.sort(np.unique([int(re.findall('\\d+', f)[0]) for f in cutoff_files]))\n",
    "l_cutoff_week_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format data for deepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"model\": int(ts['model'].values[0]), \n",
    "           \"start\": str(ts.index[0]), \n",
    "           \"target\": list(ts['y'])}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cutoff_week_id in l_cutoff_week_id[29:]:\n",
    "    \n",
    "    print(cutoff_week_id)\n",
    "    \n",
    "    train_data_cutoff = ut.read_parquet_S3(\n",
    "        bucket, \"{}/full_scope/train_data_cutoff/train_data_cutoff_{}\".format(prefix, cutoff_week_id)\n",
    "    )\n",
    "    \n",
    "    l_model = train_data_cutoff['model'].sort_values().unique()\n",
    "    l_model_test = train_data_cutoff \\\n",
    "        .loc[train_data_cutoff['week_id'] == ut.get_last_week_id(cutoff_week_id),'model'] \\\n",
    "        .sort_values() \\\n",
    "        .unique()\n",
    "    \n",
    "    nb_ts = l_model.shape[0]\n",
    "    cpt = 0\n",
    "    \n",
    "    train_ts = [] # All time series without the last 16 values\n",
    "    val_ts = []   # All time series\n",
    "    test_ts = []  # Time series to forecast for futur (=active past week)\n",
    "    \n",
    "    for m in l_model: # TO OPTIMIZE (parallel? Iterate over groupBy Object?)\n",
    "        ts = train_data_cutoff[train_data_cutoff['model'] == m].set_index('date').asfreq(freq)\n",
    "    \n",
    "        train_ts.append(ts[:-prediction_length])\n",
    "        val_ts.append(ts)\n",
    "        if m in l_model_test:\n",
    "            test_ts.append(ts)\n",
    "            \n",
    "        cpt += 1\n",
    "        if cpt % 500 == 0:\n",
    "            print(str(cpt), '/', str(nb_ts))    \n",
    "        \n",
    "    # Export to S3 in JSONL format\n",
    "    fs = s3fs.S3FileSystem()\n",
    "\n",
    "    # Train\n",
    "    with fs.open(\"{}/cutoff_{}/train.json\".format(s3_data_path, cutoff_week_id), 'wb') as fp:\n",
    "        for ts in train_ts:\n",
    "            fp.write(series_to_jsonline(ts).encode(\"utf-8\"))\n",
    "            fp.write('\\n'.encode(\"utf-8\"))\n",
    "    \n",
    "    # Val\n",
    "    with fs.open(\"{}/cutoff_{}/val.json\".format(s3_data_path, cutoff_week_id), 'wb') as fp:\n",
    "        for ts in val_ts:\n",
    "            fp.write(series_to_jsonline(ts).encode(\"utf-8\"))\n",
    "            fp.write('\\n'.encode(\"utf-8\"))\n",
    "            \n",
    "    # Test\n",
    "    with fs.open(\"{}/cutoff_{}/test.json\".format(s3_data_path, cutoff_week_id), 'wb') as fp:\n",
    "        for ts in test_ts:\n",
    "            fp.write(series_to_jsonline(ts).encode(\"utf-8\"))\n",
    "            fp.write('\\n'.encode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_forecast-modeling-demand",
   "language": "python",
   "name": "conda_forecast-modeling-demand"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
