import argparse
import boto3
import pandas as pd
import numpy as np
import re
import isoweek
from copy import deepcopy
from joblib import Parallel, delayed
import subprocess
import pickle
import os
from os import listdir
from os.path import isfile, join

from gluonts.model.prophet import ProphetPredictor


def get_next_week_id(week_id):
    '''
    ARGUMENTS:
    
    date ( integer ): week identifier in the format 'year'+'week_number'
    
    RETURNS:
    
    next week in the same format as the date argument
    '''
    if not(isinstance(week_id, (int, np.integer))):
        return 'DATE ARGUMENT NOT AN INT'
    if len(str(week_id)) != 6:
        return 'UNVALID DATE FORMAT'

    year = week_id // 100
    week = week_id % 100

    if week < 52:
        return week_id + 1
    elif week == 52:
        last_week = isoweek.Week.last_week_of_year(year).week
        if last_week == 52:
            return (week_id // 100 + 1) * 100 + 1
        elif last_week == 53:
            return week_id + 1
        else:
            return 'UNVALID ISOWEEK.LASTWEEK NUMBER'
    elif week == 53:
        if isoweek.Week.last_week_of_year(year).week == 52:
            return 'UNVALID WEEK NUMBER'
        else:
            return (date // 100 + 1) * 100 + 1
    else:
        return 'UNVALID DATE'


def get_next_n_week(week_id, n):
    next_n_week = [week_id]
    for i in range(n-1):
        week_id = get_next_week_id(week_id)
        next_n_week.append(week_id)
        
    return next_n_week


def week_id_to_date(week_id):
    assert isinstance(week_id, (int, np.integer, pd.Series))
    
    if isinstance(week_id, (int, np.integer)):
        return pd.to_datetime(str(week_id) + '-0', format='%G%V-%w') - pd.Timedelta(1, unit='W')
    else:
        return pd.to_datetime(week_id.astype(str) + '-0', format='%G%V-%w') - pd.Timedelta(1, unit='W')
    

def download_file_from_S3(bucket, s3_file_path, local_path):
    # Any clients created from this session will use credentials
    # from the [profile_name] section of ~/.aws/credentials.

    s3client = boto3.client('s3')
    s3client.download_file(bucket, s3_file_path, local_path)

    return print(
        "downloaded " +
        bucket +
        "/" +
        s3_file_path +
        " to: " +
        local_path)
        
        
def train_input_fn(train_file_path):
    with open(train_file_path, 'rb') as file:
        response = pickle.load(file)
    return response
    
    
def compute_wape(res):

    active_sales = pd.read_csv('/opt/ml/input/data/active_sales.csv', sep='|', parse_dates=['date'])
                              
    res = pd.merge(res, active_sales, how="left")
    res["ae"] = np.abs(res["yhat"] - res["y"])
        
    cutoff_abs_error = res["ae"].sum()
    cutoff_target_sum = res["y"].sum()
    
    return cutoff_abs_error, cutoff_target_sum


    
def model_fn(cutoff_week_id, hyperparameters):

    train = train_input_fn(train_dir + '/gluonts_ds_cutoff_' + str(cutoff_week_id) + '.pkl')
        
    nb_ts = len(train)
    
    def configure_seasonality(model):
        model.add_seasonality(
            name='yearly', 
            period=365.25, 
            fourier_order=hyperparameters['yearly_order'],
        )
        model.add_seasonality(
            name='quaterly',
            period=365.25/2, 
            fourier_order=hyperparameters['quaterly_order'], 
        )
        return model

    estimator = ProphetPredictor(
        freq=prediction_freq,
        prediction_length=prediction_length,
        prophet_params={'weekly_seasonality' : False,
                        'daily_seasonality' : False,
                        'yearly_seasonality' : False,
                        'n_changepoints' : hyperparameters['n_changepoints'],
                        'changepoint_range' : hyperparameters['changepoint_range'],
                        'changepoint_prior_scale' : hyperparameters['changepoint_prior_scale'],
                        'seasonality_prior_scale' : hyperparameters['seasonality_prior_scale']},
        init_model=configure_seasonality)
    
    predictor = estimator.predict(train)
    forecasts = list(predictor)
    
    week_id_range = get_next_n_week(cutoff_week_id, horizon)
    
    res = pd.DataFrame(
        {'cutoff_week_id' : cutoff_week_id,
         'cutoff_date' : week_id_to_date(cutoff_week_id),
         'week_id' : week_id_range * nb_ts,
         'date' : [week_id_to_date(w) for w in week_id_range] * nb_ts,
         'model' : np.array([np.repeat(x['model'], prediction_length) for x in train]).flatten(),
         'yhat' : np.array([x.quantile(0.5).round().astype(int) for x in forecasts]).flatten()})
    
    res.loc[res['yhat'] < 0, 'yhat'] = 0
    
    res.to_csv(model_dir + '/Facebook_Prophet_cutoff_' + str(cutoff_week_id) + '.csv')
    
    return compute_wape(res)
    
    
def train_model_fn(cutoff_files_path, hyperparameters, max_jobs=-1, only_last=True):
                                    
    cutoff_files = [f for f in listdir(cutoff_files_path) if isfile(join(cutoff_files_path, f))]

    cutoff_weeks = np.array([int(re.findall('\d+', f)[0]) for f in cutoff_files])
    
    if only_last:
        cutoff_weeks = np.array([np.max(cutoff_weeks)])

    if max_jobs <= 0:
        max_jobs = len(cutoff_weeks)
        
    all_res = Parallel(n_jobs=max_jobs, verbose=1)\
            (delayed(model_fn)(cutoff_week_id, hyperparameters) for cutoff_week_id in cutoff_weeks)
            
    l_cutoff_abs_error = [x[0] for x in all_res]
    l_cutoff_target_sum = [x[1] for x in all_res]
    
    l_cutoff_wape = np.array(l_cutoff_abs_error) / np.array(l_cutoff_target_sum)
    global_wape = np.sum(l_cutoff_abs_error) / np.sum(l_cutoff_target_sum)
    
    print("\n--------------------------------\n")
    print("cutoff_wape:", str(l_cutoff_wape))
    print("global_wape:", str(global_wape))


def parse_args():

    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to the script
    parser.add_argument('--yearly_order', type=int, default=27)
    parser.add_argument('--quaterly_order', type=int, default=5)
    
    parser.add_argument('--weekly_seasonality', type=bool, default=False)
    parser.add_argument('--daily_seasonality', type=bool, default=False)
    parser.add_argument('--yearly_seasonality', type=bool, default=False)
    
    parser.add_argument('--n_changepoints', type=int, default=36)
    parser.add_argument('--changepoint_range', type=float, default=0.69)
    parser.add_argument('--changepoint_prior_scale', type=float, default=1.91)
    parser.add_argument('--seasonality_prior_scale', type=float, default=2.04)
    
    # data, model, and output directories
    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))
    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))

    return parser.parse_known_args()

    
if __name__ == "__main__":

    args, _ = parse_args()
    
    hyperparameters = {'yearly_order': args.yearly_order,
                       'quaterly_order': args.quaterly_order,
                       'weekly_seasonality': args.weekly_seasonality,
                       'daily_seasonality': args.daily_seasonality,
                       'yearly_seasonality': args.yearly_seasonality,
                       'n_changepoints': args.n_changepoints,
                       'changepoint_range': args.changepoint_range,
                       'changepoint_prior_scale': args.changepoint_prior_scale,
                       'seasonality_prior_scale': args.seasonality_prior_scale}
                       
    print(hyperparameters)
    print(args.train, args.model_dir)
    print(type(hyperparameters['seasonality_prior_scale']))
    
    train_dir = '/opt/ml/input/data/training' # args.train #
    model_dir = '/opt/ml/model' # args.model_dir #
    
    horizon = 10
    horizon_freq = '1W-SUN'

    prediction_length = horizon
    prediction_freq = '1W-SUN'
    season_length = 52
    
    download_file_from_S3('fcst-workspace',
                          'qlik/data/clean/active_sales.csv',
                          '/opt/ml/input/data/active_sales.csv')                               
    
    #print(subprocess.run(["ls", train_dir]))
    #train_input = train_input_fn(train_dir + '/gluonts_ds_cutoff_201922.pkl')
    #forecast_with_prophet(cutoff_files_path='gluonts_data/', max_jobs=-1, only_last=True)
    
    train_model_fn(train_dir, hyperparameters)